{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ptr_decoder import pointer_decoder\n",
    "from pointer_network import PointerNetwork\n",
    "from data_generator import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "hidden_dim = 100\n",
    "lr = 0.001\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointer_net = PointerNetwork(hidden_dim, lr, 5, seq_len, batch_size=batch_size, input_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = pointer_net.train_op\n",
    "loss = pointer_net.loss\n",
    "pred = pointer_net.predictions\n",
    "decoder_outputs = pointer_net.decoder_outputs\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 0 7 3 4 7 2 8]\n",
      "[8 8 8 8 8 8 8 8 8]\n",
      "Loss 25.6083\n",
      "[1 6 6 3 4 5 0 7 8]\n",
      "[1 8 8 8 8 8 8 8 8]\n",
      "Loss 3.10769\n",
      "[1 0 6 6 0 2 5 5 8]\n",
      "[0 0 0 7 8 8 8 8 8]\n",
      "Loss 3.07048\n",
      "[3 3 7 6 2 2 6 5 8]\n",
      "[0 0 0 8 8 8 8 8 8]\n",
      "Loss 2.96422\n",
      "[2 5 0 6 6 7 3 3 8]\n",
      "[6 5 4 0 8 8 8 8 8]\n",
      "Loss 2.91899\n",
      "[5 4 3 7 1 5 0 3 8]\n",
      "[5 5 5 1 8 8 8 8 8]\n",
      "Loss 2.81097\n",
      "[3 0 4 2 0 7 2 6 8]\n",
      "[6 6 6 6 8 8 8 8 8]\n",
      "Loss 2.70599\n",
      "[5 3 0 1 3 2 6 5 8]\n",
      "[5 5 5 6 8 8 8 8 8]\n",
      "Loss 2.75207\n",
      "[7 7 2 6 3 1 6 3 8]\n",
      "[6 6 6 4 8 8 8 8 8]\n",
      "Loss 2.68255\n",
      "[4 7 1 1 5 4 7 6 8]\n",
      "[6 6 6 2 8 8 8 8 8]\n",
      "Loss 2.73916\n",
      "[4 6 6 7 3 1 2 0 8]\n",
      "[6 6 6 6 8 8 8 8 8]\n",
      "Loss 2.73892\n",
      "[5 5 4 6 3 7 0 4 8]\n",
      "[7 3 2 0 8 8 8 8 8]\n",
      "Loss 2.68974\n",
      "[2 5 7 2 5 7 4 3 8]\n",
      "[7 2 7 7 8 8 8 8 8]\n",
      "Loss 2.64207\n",
      "[2 1 3 5 0 2 5 7 8]\n",
      "[0 0 0 4 8 8 8 8 8]\n",
      "Loss 2.70168\n",
      "[5 4 4 5 7 2 2 3 8]\n",
      "[7 2 2 7 8 8 8 8 8]\n",
      "Loss 2.72995\n",
      "[5 0 0 3 2 5 6 2 8]\n",
      "[5 0 0 4 8 8 8 8 8]\n",
      "Loss 2.67354\n",
      "[0 2 7 6 3 3 1 4 8]\n",
      "[7 7 7 7 8 8 8 8 8]\n",
      "Loss 2.72896\n",
      "[1 5 0 4 5 3 6 7 8]\n",
      "[6 6 7 2 8 8 8 8 8]\n",
      "Loss 2.67322\n",
      "[0 3 5 2 7 5 3 6 8]\n",
      "[0 0 0 2 8 8 8 8 8]\n",
      "Loss 2.61044\n",
      "[6 4 2 3 3 5 0 2 8]\n",
      "[3 3 6 4 8 8 8 8 8]\n",
      "Loss 2.88468\n",
      "[4 3 2 7 6 6 5 5 8]\n",
      "[6 6 6 6 8 8 8 8 8]\n",
      "Loss 2.66253\n",
      "[7 5 3 2 4 1 1 4 8]\n",
      "[7 0 7 0 8 8 8 8 8]\n",
      "Loss 2.66213\n",
      "[0 3 2 4 3 2 5 0 8]\n",
      "[7 7 7 4 8 8 8 8 8]\n",
      "Loss 2.66143\n",
      "[3 4 1 1 2 2 7 3 8]\n",
      "[4 4 4 4 8 8 8 8 8]\n",
      "Loss 2.64657\n",
      "[6 3 1 4 4 7 7 6 8]\n",
      "[6 6 6 3 8 8 8 8 8]\n",
      "Loss 2.69236\n",
      "[6 4 2 1 0 7 1 0 8]\n",
      "[7 5 7 0 8 8 8 8 8]\n",
      "Loss 2.59379\n",
      "[7 3 6 0 1 5 2 2 8]\n",
      "[7 1 1 1 8 8 8 8 8]\n",
      "Loss 2.65461\n",
      "[4 2 1 1 6 5 3 0 8]\n",
      "[7 0 7 6 8 8 8 8 8]\n",
      "Loss 2.70199\n",
      "[0 3 4 6 2 2 5 4 8]\n",
      "[7 4 2 2 8 8 8 8 8]\n",
      "Loss 2.70006\n",
      "[7 7 0 3 6 6 5 0 8]\n",
      "[7 0 0 2 8 8 8 8 8]\n",
      "Loss 2.6906\n",
      "[3 1 6 7 4 4 1 3 8]\n",
      "[3 3 5 6 8 8 8 8 8]\n",
      "Loss 2.70618\n",
      "[4 7 3 3 7 0 6 1 8]\n",
      "[4 4 5 1 8 8 8 8 8]\n",
      "Loss 2.65385\n",
      "[5 5 6 1 2 6 0 7 8]\n",
      "[5 5 2 1 8 8 8 8 8]\n",
      "Loss 2.67333\n",
      "[0 4 5 3 3 6 7 2 8]\n",
      "[2 2 2 3 8 8 8 8 8]\n",
      "Loss 2.65453\n",
      "[1 5 4 2 6 6 7 4 8]\n",
      "[4 7 7 2 8 8 8 8 8]\n",
      "Loss 2.69718\n",
      "[6 3 5 2 1 6 4 1 8]\n",
      "[7 7 0 2 8 8 8 8 8]\n",
      "Loss 2.69752\n",
      "[4 0 0 1 2 6 5 5 8]\n",
      "[0 0 6 7 8 8 8 8 8]\n",
      "Loss 2.65598\n",
      "[6 6 2 2 3 5 0 3 8]\n",
      "[2 2 2 4 8 8 8 8 8]\n",
      "Loss 2.67019\n",
      "[7 1 2 2 3 3 4 0 8]\n",
      "[7 2 7 5 8 8 8 8 8]\n",
      "Loss 2.68685\n",
      "[2 7 1 6 4 3 3 4 8]\n",
      "[1 1 0 3 8 8 8 8 8]\n",
      "Loss 2.86261\n",
      "[1 4 3 0 5 1 4 5 8]\n",
      "[3 3 3 3 8 8 8 8 8]\n",
      "Loss 2.65198\n",
      "[1 0 0 2 7 6 3 3 8]\n",
      "[0 0 2 5 8 8 8 8 8]\n",
      "Loss 2.67685\n",
      "[3 1 0 3 5 5 6 6 8]\n",
      "[0 4 4 4 8 8 8 8 8]\n",
      "Loss 2.65889\n",
      "[2 1 6 7 7 0 3 6 8]\n",
      "[6 6 7 0 8 8 8 8 8]\n",
      "Loss 2.66644\n",
      "[6 6 1 1 0 7 2 4 8]\n",
      "[0 0 6 6 8 8 8 8 8]\n",
      "Loss 2.7473\n",
      "[6 5 7 7 5 1 3 2 8]\n",
      "[3 3 3 1 8 8 8 8 8]\n",
      "Loss 2.69722\n",
      "[3 3 7 1 5 7 0 5 8]\n",
      "[7 3 3 2 8 8 8 8 8]\n",
      "Loss 2.69192\n",
      "[1 0 5 7 0 1 7 6 8]\n",
      "[0 4 4 4 8 8 8 8 8]\n",
      "Loss 2.70846\n",
      "[7 6 6 3 2 0 3 7 8]\n",
      "[6 6 6 0 8 8 8 8 8]\n",
      "Loss 2.67768\n",
      "[6 6 3 1 0 3 1 7 8]\n",
      "[0 0 0 4 8 8 8 8 8]\n",
      "Loss 2.67996\n",
      "[1 3 4 2 2 1 7 7 8]\n",
      "[1 1 1 3 8 8 8 8 8]\n",
      "Loss 2.74059\n",
      "[4 7 2 2 5 1 0 0 8]\n",
      "[0 0 7 7 8 8 8 8 8]\n",
      "Loss 2.69236\n",
      "[0 1 4 5 1 0 7 4 8]\n",
      "[0 0 0 3 8 8 8 8 8]\n",
      "Loss 2.64439\n",
      "[5 0 2 3 3 4 6 6 8]\n",
      "[2 0 0 7 8 8 8 8 8]\n",
      "Loss 2.55886\n",
      "[7 4 2 1 1 6 0 7 8]\n",
      "[6 6 7 1 8 8 8 8 8]\n",
      "Loss 2.66063\n",
      "[7 7 5 4 3 1 4 0 8]\n",
      "[7 3 3 1 8 8 8 8 8]\n",
      "Loss 2.61792\n",
      "[0 4 4 0 7 7 1 3 8]\n",
      "[4 4 0 0 8 8 8 8 8]\n",
      "Loss 2.6512\n",
      "[4 5 1 4 2 1 3 7 8]\n",
      "[0 0 6 6 8 8 8 8 8]\n",
      "Loss 2.59639\n",
      "[0 2 1 3 7 1 5 7 8]\n",
      "[5 5 2 2 8 8 8 8 8]\n",
      "Loss 2.68508\n",
      "[3 4 2 2 1 0 0 5 8]\n",
      "[1 1 6 6 8 8 8 8 8]\n",
      "Loss 2.72302\n",
      "[4 3 0 4 5 2 7 1 8]\n",
      "[4 4 7 7 8 8 8 8 8]\n",
      "Loss 2.62666\n",
      "[6 1 2 7 3 0 4 3 8]\n",
      "[2 0 7 7 8 8 8 8 8]\n",
      "Loss 2.64524\n",
      "[3 0 0 4 2 7 7 5 8]\n",
      "[0 1 1 4 8 8 8 8 8]\n",
      "Loss 2.70006\n",
      "[3 2 5 1 7 5 1 7 8]\n",
      "[7 7 7 2 8 8 8 8 8]\n",
      "Loss 2.74427\n",
      "[5 6 3 0 4 4 6 1 8]\n",
      "[3 3 0 0 8 8 8 8 8]\n",
      "Loss 2.69084\n",
      "[0 0 1 7 5 4 6 3 8]\n",
      "[0 7 7 3 8 8 8 8 8]\n",
      "Loss 2.65277\n",
      "[0 3 7 5 3 4 5 1 8]\n",
      "[3 0 0 1 8 8 8 8 8]\n",
      "Loss 2.73777\n",
      "[5 3 7 4 3 7 0 2 8]\n",
      "[3 7 4 4 8 8 8 8 8]\n",
      "Loss 2.67201\n",
      "[3 1 6 3 5 0 0 6 8]\n",
      "[3 3 2 1 8 8 8 8 8]\n",
      "Loss 2.66669\n",
      "[5 5 3 7 6 1 2 2 8]\n",
      "[0 6 7 7 8 8 8 8 8]\n",
      "Loss 2.73567\n",
      "[6 5 3 4 0 2 1 6 8]\n",
      "[6 6 7 7 8 8 8 8 8]\n",
      "Loss 2.6974\n",
      "[0 4 2 7 6 6 3 3 8]\n",
      "[0 0 0 7 8 8 8 8 8]\n",
      "Loss 2.62504\n",
      "[0 1 4 4 5 5 3 7 8]\n",
      "[7 7 7 1 8 8 8 8 8]\n",
      "Loss 2.69598\n",
      "[1 1 5 4 2 6 4 5 8]\n",
      "[0 0 6 6 8 8 8 8 8]\n",
      "Loss 2.65896\n",
      "[1 7 5 4 4 2 7 3 8]\n",
      "[7 7 7 7 8 8 8 8 8]\n",
      "Loss 2.63634\n",
      "[1 1 4 4 0 3 5 2 8]\n",
      "[0 7 7 5 8 8 8 8 8]\n",
      "Loss 2.59296\n",
      "[4 2 2 3 3 0 5 7 8]\n",
      "[4 2 2 0 8 8 8 8 8]\n",
      "Loss 2.64541\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10000):\n",
    "    _encoder_inps, _targets, _decoder_inps = sorting_generator(seq_len, batch_size)\n",
    "    # _encoder_inps, _decoder_inps, _targets = data.next_batch(2, 8)\n",
    "    feed_dict = pointer_net.feed_dict(decoder_inpt_data=_decoder_inps, \n",
    "                                      encoder_inpt_data=_encoder_inps, target_data=_targets)\n",
    "\n",
    "    _loss, _, _vs = sess.run([loss, opt, pred], feed_dict=feed_dict)\n",
    "    if i%100 == 0:    \n",
    "        print np.argmax(np.array(_targets)[:, 0, :], axis=1)\n",
    "        print np.argmax(np.array(_vs)[:, 0, :], axis=1)\n",
    "        print \"Loss\", _loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Construct a DataGenerator.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def next_batch(self, batch_size, N, train_mode=True):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "\n",
    "        # A sequence of random numbers from [0, 1]\n",
    "        reader_input_batch = []\n",
    "\n",
    "        # Sorted sequence that we feed to encoder\n",
    "        # In inference we feed an unordered sequence again\n",
    "        decoder_input_batch = []\n",
    "\n",
    "        # Ordered sequence where one hot vector encodes position in the input array\n",
    "        writer_outputs_batch = []\n",
    "        for _ in range(N):\n",
    "            reader_input_batch.append(np.zeros([batch_size, 1]))\n",
    "        for _ in range(N + 1):\n",
    "            decoder_input_batch.append(np.zeros([batch_size, 1]))\n",
    "            writer_outputs_batch.append(np.zeros([batch_size, N + 1]))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            shuffle = np.random.permutation(N)\n",
    "            sequence = np.sort(np.random.random(N))\n",
    "            shuffled_sequence = sequence[shuffle]\n",
    "\n",
    "            for i in range(N):\n",
    "                reader_input_batch[i][b] = shuffled_sequence[i]\n",
    "                if train_mode:\n",
    "                    decoder_input_batch[i + 1][b] = sequence[i]\n",
    "                else:\n",
    "                    decoder_input_batch[i + 1][b] = shuffled_sequence[i]\n",
    "                writer_outputs_batch[shuffle[i]][b, i + 1] = 1.0\n",
    "\n",
    "            # Points to the stop symbol\n",
    "            writer_outputs_batch[N][b, -1] = 1.0\n",
    "\n",
    "        return reader_input_batch, decoder_input_batch, writer_outputs_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
