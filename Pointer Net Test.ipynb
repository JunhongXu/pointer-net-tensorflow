{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ptr_decoder import pointer_decoder\n",
    "from pointer_network import PointerNetwork\n",
    "from data_generator import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "batch_size = 32\n",
    "seq_len = 8\n",
    "hidden_dim = 128\n",
    "lr = 0.01\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointer_net = PointerNetwork(hidden_dim, lr, 10, sort, \n",
    "                             max_seq_len=seq_len, batch_size=batch_size, input_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Testing-------------------\n",
      "Epoch 0:\n",
      "Sample:\n",
      "ground truth: [ 0.04019927  0.06430691  0.22457752  0.56303015  0.64774744  0.70851185\n",
      "  0.79847057  0.85636531]\n",
      "prediction [ 0.22457752  0.22457752  0.22457752  0.22457752  0.22457752  0.22457752\n",
      "  0.22457752  0.22457752]\n",
      "Test loss is 18.1174\n",
      "Test accuracy is 0.0\n",
      "--------------------Done--------------------\n",
      "Epoch 0: training loss is 17.9078\n",
      "Epoch 50: training loss is 15.1607\n",
      "------------------Testing-------------------\n",
      "Epoch 100:\n",
      "Sample:\n",
      "ground truth: [ 0.5252165   0.59039069  0.73010986  0.7428869   0.7490249   0.78165124\n",
      "  0.82920861  0.97163153]\n",
      "prediction [ 0.5252165   0.5252165   0.5252165   0.7490249   0.97163153  0.97163153\n",
      "  0.97163153  0.97163153]\n",
      "Test loss is 11.4928\n",
      "Test accuracy is 0.0\n",
      "--------------------Done--------------------\n",
      "Epoch 100: training loss is 11.4639\n",
      "Epoch 150: training loss is 8.80549\n",
      "------------------Testing-------------------\n",
      "Epoch 200:\n",
      "Sample:\n",
      "ground truth: [ 0.12432993  0.21550984  0.38034912  0.47974136  0.51659704  0.66778498\n",
      "  0.71930784  0.94511883]\n",
      "prediction [ 0.12432993  0.21550984  0.38034912  0.47974136  0.51659704  0.71930784\n",
      "  0.94511883  0.94511883]\n",
      "Test loss is 6.60542\n",
      "Test accuracy is 0.09375\n",
      "--------------------Done--------------------\n",
      "Epoch 200: training loss is 7.54085\n",
      "Epoch 250: training loss is 6.29233\n",
      "------------------Testing-------------------\n",
      "Epoch 300:\n",
      "Sample:\n",
      "ground truth: [ 0.02985246  0.03215928  0.11181984  0.19064828  0.61259452  0.64942243\n",
      "  0.85768596  0.9794991 ]\n",
      "prediction [ 0.03215928  0.11181984  0.19064828  0.19064828  0.61259452  0.64942243\n",
      "  0.85768596  0.9794991 ]\n",
      "Test loss is 6.26479\n",
      "Test accuracy is 0.03125\n",
      "--------------------Done--------------------\n",
      "Epoch 300: training loss is 5.75189\n",
      "Epoch 350: training loss is 5.02776\n",
      "------------------Testing-------------------\n",
      "Epoch 400:\n",
      "Sample:\n",
      "ground truth: [ 0.23931182  0.31387605  0.39496683  0.53221106  0.59993233  0.73029956\n",
      "  0.86307996  0.92563292]\n",
      "prediction [ 0.23931182  0.31387605  0.39496683  0.53221106  0.59993233  0.73029956\n",
      "  0.86307996  0.92563292]\n",
      "Test loss is 4.83188\n",
      "Test accuracy is 0.34375\n",
      "--------------------Done--------------------\n",
      "Epoch 400: training loss is 5.34074\n",
      "Epoch 450: training loss is 4.16379\n",
      "------------------Testing-------------------\n",
      "Epoch 500:\n",
      "Sample:\n",
      "ground truth: [ 0.04618732  0.18462387  0.30175996  0.64852128  0.72186851  0.75608318\n",
      "  0.76628761  0.96153221]\n",
      "prediction [ 0.04618732  0.18462387  0.30175996  0.64852128  0.72186851  0.76628761\n",
      "  0.76628761  0.96153221]\n",
      "Test loss is 4.4041\n",
      "Test accuracy is 0.1875\n",
      "--------------------Done--------------------\n",
      "Epoch 500: training loss is 4.28906\n",
      "Epoch 550: training loss is 3.44791\n",
      "------------------Testing-------------------\n",
      "Epoch 600:\n",
      "Sample:\n",
      "ground truth: [ 0.00307024  0.00752883  0.34062228  0.42713022  0.46262464  0.48350235\n",
      "  0.52642741  0.61448306]\n",
      "prediction [ 0.00752883  0.00752883  0.34062228  0.34062228  0.42713022  0.48350235\n",
      "  0.52642741  0.61448306]\n",
      "Test loss is 4.21424\n",
      "Test accuracy is 0.21875\n",
      "--------------------Done--------------------\n",
      "Epoch 600: training loss is 4.1747\n",
      "Epoch 650: training loss is 3.63765\n",
      "------------------Testing-------------------\n",
      "Epoch 700:\n",
      "Sample:\n",
      "ground truth: [ 0.13067939  0.40641067  0.48769601  0.66477492  0.71642901  0.9004781\n",
      "  0.91836239  0.94968786]\n",
      "prediction [ 0.13067939  0.40641067  0.48769601  0.66477492  0.71642901  0.9004781\n",
      "  0.91836239  0.94968786]\n",
      "Test loss is 3.19204\n",
      "Test accuracy is 0.25\n",
      "--------------------Done--------------------\n",
      "Epoch 700: training loss is 2.6396\n",
      "Epoch 750: training loss is 3.16406\n",
      "------------------Testing-------------------\n",
      "Epoch 800:\n",
      "Sample:\n",
      "ground truth: [ 0.10322456  0.24316021  0.5361242   0.58432813  0.63814476  0.66488028\n",
      "  0.85134622  0.98785989]\n",
      "prediction [ 0.10322456  0.24316021  0.5361242   0.58432813  0.66488028  0.66488028\n",
      "  0.85134622  0.98785989]\n",
      "Test loss is 3.45686\n",
      "Test accuracy is 0.15625\n",
      "--------------------Done--------------------\n",
      "Epoch 800: training loss is 2.31591\n",
      "Epoch 850: training loss is 2.38567\n",
      "------------------Testing-------------------\n",
      "Epoch 900:\n",
      "Sample:\n",
      "ground truth: [ 0.07067808  0.25665364  0.48477536  0.50652108  0.65486429  0.76339736\n",
      "  0.8169276   0.87041996]\n",
      "prediction [ 0.07067808  0.25665364  0.48477536  0.50652108  0.65486429  0.76339736\n",
      "  0.87041996  0.87041996]\n",
      "Test loss is 2.41987\n",
      "Test accuracy is 0.40625\n",
      "--------------------Done--------------------\n",
      "Epoch 900: training loss is 1.93987\n",
      "Epoch 950: training loss is 2.95093\n",
      "------------------Testing-------------------\n",
      "Epoch 1000:\n",
      "Sample:\n",
      "ground truth: [ 0.074058    0.19169603  0.25052555  0.26044614  0.3658008   0.59852352\n",
      "  0.9726621   0.97734027]\n",
      "prediction [ 0.074058    0.19169603  0.25052555  0.3658008   0.3658008   0.59852352\n",
      "  0.9726621   0.97734027]\n",
      "Test loss is 2.90748\n",
      "Test accuracy is 0.25\n",
      "--------------------Done--------------------\n",
      "Epoch 1000: training loss is 3.19487\n",
      "Epoch 1050: training loss is 2.26778\n",
      "------------------Testing-------------------\n",
      "Epoch 1100:\n",
      "Sample:\n",
      "ground truth: [ 0.44763399  0.59031761  0.65557164  0.762891    0.79401043  0.8155174\n",
      "  0.84482912  0.89459245]\n",
      "prediction [ 0.44763399  0.59031761  0.65557164  0.762891    0.762891    0.8155174\n",
      "  0.89459245  0.89459245]\n",
      "Test loss is 1.92089\n",
      "Test accuracy is 0.5625\n",
      "--------------------Done--------------------\n",
      "Epoch 1100: training loss is 2.15762\n",
      "Epoch 1150: training loss is 1.69469\n",
      "------------------Testing-------------------\n",
      "Epoch 1200:\n",
      "Sample:\n",
      "ground truth: [ 0.34848609  0.39093113  0.39706334  0.63363976  0.87746474  0.94387249\n",
      "  0.97863239  0.99854559]\n",
      "prediction [ 0.34848609  0.39706334  0.39093113  0.63363976  0.87746474  0.87746474\n",
      "  0.94387249  0.97863239]\n",
      "Test loss is 2.68218\n",
      "Test accuracy is 0.46875\n",
      "--------------------Done--------------------\n",
      "Epoch 1200: training loss is 2.65566\n",
      "Epoch 1250: training loss is 1.90392\n",
      "------------------Testing-------------------\n",
      "Epoch 1300:\n",
      "Sample:\n",
      "ground truth: [ 0.00210386  0.05846282  0.09477262  0.10888165  0.13597039  0.38844859\n",
      "  0.63085783  0.96343959]\n",
      "prediction [ 0.00210386  0.00210386  0.05846282  0.09477262  0.13597039  0.38844859\n",
      "  0.63085783  0.96343959]\n",
      "Test loss is 2.25106\n",
      "Test accuracy is 0.4375\n",
      "--------------------Done--------------------\n",
      "Epoch 1300: training loss is 2.38263\n",
      "Epoch 1350: training loss is 2.34052\n",
      "------------------Testing-------------------\n",
      "Epoch 1400:\n",
      "Sample:\n",
      "ground truth: [ 0.08241021  0.17959013  0.34341376  0.34608378  0.47085876  0.57840095\n",
      "  0.86173989  0.93628652]\n",
      "prediction [ 0.08241021  0.17959013  0.34341376  0.34608378  0.47085876  0.57840095\n",
      "  0.86173989  0.93628652]\n",
      "Test loss is 3.32399\n",
      "Test accuracy is 0.40625\n",
      "--------------------Done--------------------\n",
      "Epoch 1400: training loss is 1.83711\n",
      "Epoch 1450: training loss is 1.70543\n",
      "------------------Testing-------------------\n",
      "Epoch 1500:\n",
      "Sample:\n",
      "ground truth: [ 0.09620824  0.47170133  0.72547591  0.72565385  0.72667741  0.8059542\n",
      "  0.81713142  0.97718225]\n",
      "prediction [ 0.09620824  0.47170133  0.72667741  0.72667741  0.72667741  0.8059542\n",
      "  0.81713142  0.97718225]\n",
      "Test loss is 1.91424\n",
      "Test accuracy is 0.625\n",
      "--------------------Done--------------------\n",
      "Epoch 1500: training loss is 1.76303\n",
      "Epoch 1550: training loss is 1.78339\n",
      "------------------Testing-------------------\n",
      "Epoch 1600:\n",
      "Sample:\n",
      "ground truth: [ 0.0237012   0.07853297  0.30932327  0.39385449  0.66880781  0.72260395\n",
      "  0.74186475  0.84707924]\n",
      "prediction [ 0.0237012   0.07853297  0.30932327  0.39385449  0.66880781  0.72260395\n",
      "  0.74186475  0.84707924]\n",
      "Test loss is 1.66859\n",
      "Test accuracy is 0.59375\n",
      "--------------------Done--------------------\n",
      "Epoch 1600: training loss is 1.469\n",
      "Epoch 1650: training loss is 2.07415\n",
      "------------------Testing-------------------\n",
      "Epoch 1700:\n",
      "Sample:\n",
      "ground truth: [ 0.06307587  0.09251676  0.30131659  0.40115563  0.56287925  0.72252077\n",
      "  0.81809182  0.90028482]\n",
      "prediction [ 0.06307587  0.09251676  0.30131659  0.40115563  0.56287925  0.72252077\n",
      "  0.81809182  0.90028482]\n",
      "Test loss is 1.54626\n",
      "Test accuracy is 0.625\n",
      "--------------------Done--------------------\n",
      "Epoch 1700: training loss is 1.60577\n",
      "Epoch 1750: training loss is 1.70181\n",
      "------------------Testing-------------------\n",
      "Epoch 1800:\n",
      "Sample:\n",
      "ground truth: [ 0.01062507  0.02074112  0.04523445  0.15292474  0.32667974  0.45778798\n",
      "  0.50015232  0.70076205]\n",
      "prediction [ 0.01062507  0.02074112  0.04523445  0.15292474  0.32667974  0.45778798\n",
      "  0.50015232  0.70076205]\n",
      "Test loss is 1.77737\n",
      "Test accuracy is 0.65625\n",
      "--------------------Done--------------------\n",
      "Epoch 1800: training loss is 2.13095\n",
      "Epoch 1850: training loss is 2.35287\n",
      "------------------Testing-------------------\n",
      "Epoch 1900:\n",
      "Sample:\n",
      "ground truth: [ 0.28309073  0.33211692  0.37154006  0.3819897   0.51118717  0.60558297\n",
      "  0.78050369  0.80558358]\n",
      "prediction [ 0.28309073  0.28309073  0.37154006  0.3819897   0.51118717  0.60558297\n",
      "  0.78050369  0.80558358]\n",
      "Test loss is 1.84338\n",
      "Test accuracy is 0.5625\n",
      "--------------------Done--------------------\n",
      "Epoch 1900: training loss is 1.554\n",
      "Epoch 1950: training loss is 1.13834\n",
      "------------------Testing-------------------\n",
      "Epoch 2000:\n",
      "Sample:\n",
      "ground truth: [ 0.05613875  0.21942879  0.22055014  0.44637923  0.56229705  0.72896221\n",
      "  0.73913735  0.95913961]\n",
      "prediction [ 0.05613875  0.05613875  0.21942879  0.44637923  0.56229705  0.72896221\n",
      "  0.73913735  0.95913961]\n",
      "Test loss is 1.60382\n",
      "Test accuracy is 0.53125\n",
      "--------------------Done--------------------\n",
      "Epoch 2000: training loss is 2.04853\n",
      "Epoch 2050: training loss is 2.20091\n",
      "------------------Testing-------------------\n",
      "Epoch 2100:\n",
      "Sample:\n",
      "ground truth: [ 0.11041979  0.312909    0.38088058  0.46433001  0.54526537  0.62285396\n",
      "  0.68706023  0.85295711]\n",
      "prediction [ 0.11041979  0.312909    0.38088058  0.46433001  0.54526537  0.62285396\n",
      "  0.68706023  0.85295711]\n",
      "Test loss is 1.37192\n",
      "Test accuracy is 0.5625\n",
      "--------------------Done--------------------\n",
      "Epoch 2100: training loss is 1.59845\n",
      "Epoch 2150: training loss is 1.57138\n",
      "------------------Testing-------------------\n",
      "Epoch 2200:\n",
      "Sample:\n",
      "ground truth: [ 0.04706004  0.29654544  0.41106338  0.44544932  0.51954388  0.62029918\n",
      "  0.84990911  0.94874784]\n",
      "prediction [ 0.04706004  0.29654544  0.41106338  0.44544932  0.51954388  0.62029918\n",
      "  0.84990911  0.94874784]\n",
      "Test loss is 1.90268\n",
      "Test accuracy is 0.65625\n",
      "--------------------Done--------------------\n",
      "Epoch 2200: training loss is 1.54328\n",
      "Epoch 2250: training loss is 1.1908\n",
      "------------------Testing-------------------\n",
      "Epoch 2300:\n",
      "Sample:\n",
      "ground truth: [ 0.03049705  0.23292226  0.24612362  0.29717664  0.34325806  0.48859949\n",
      "  0.57822704  0.91989027]\n",
      "prediction [ 0.03049705  0.23292226  0.24612362  0.29717664  0.34325806  0.48859949\n",
      "  0.57822704  0.91989027]\n",
      "Test loss is 1.76151\n",
      "Test accuracy is 0.59375\n",
      "--------------------Done--------------------\n",
      "Epoch 2300: training loss is 1.66509\n"
     ]
    }
   ],
   "source": [
    "pointer_net.train(sess, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(x, y):\n",
    "    return np.linalg.norm(np.asarray(x) - np.asarray(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_tsp_dynamic(points):\n",
    "    # calc all lengths\n",
    "    all_distances = [[length(x, y) for y in points] for x in points]\n",
    "    # initial value - just distance from 0 to every other point + keep the track of edges\n",
    "    A = {(frozenset([0, idx + 1]), idx + 1): (dist, [0, idx + 1]) for idx, dist in enumerate(all_distances[0][1:])}\n",
    "    cnt = len(points)\n",
    "    for m in range(2, cnt):\n",
    "        B = {}\n",
    "        for S in [frozenset(C) | {0} for C in itertools.combinations(range(1, cnt), m)]:\n",
    "            for j in S - {0}:\n",
    "                # this will use 0th index of tuple for ordering, the same as if key=itemgetter(0) used\n",
    "                B[(S, j)] = min([(A[(S - {j}, k)][0] + all_distances[k][j], A[(S - {j}, k)][1] + [j])\n",
    "                                 for k in S if k != 0 and k != j])\n",
    "        A = B\n",
    "    res = min([(A[d][0] + all_distances[0][d[1]], A[d][1]) for d in iter(A)])\n",
    "    rres = res[1]\n",
    "    # rres.append(0)\n",
    "    rres = np.asarray(rres)\n",
    "    return rres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = np.random.random(size=(5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.25865835743826232, 0.46144456844717519, 0.6995954244319661, 0.77011921536157579], [0.25865835743826232, 0.0, 0.20694693216422411, 0.93006376984076411, 0.81963941593632372], [0.46144456844717519, 0.20694693216422411, 0.0, 1.0995023274576792, 0.86151055829477785], [0.6995954244319661, 0.93006376984076411, 1.0995023274576792, 0.0, 0.70211444752843966], [0.77011921536157579, 0.81963941593632372, 0.86151055829477785, 0.70211444752843966, 0.0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 3, 4, 2, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_tsp_dynamic(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1106b70d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAFkCAYAAACThxm6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f29c3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(nodes[:, ::2], nodes[:, 1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sort'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-p3.5]",
   "language": "python",
   "name": "conda-env-tensorflow-p3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
